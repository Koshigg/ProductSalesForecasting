# -*- coding: utf-8 -*-
"""Product_Sales_Forecasting_KousalyaR

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QP3894dSvGzKs5MglBRkVlBgQrUQ8nv6
"""

!pip install xgboost

!pip uninstall -y numpy pmdarima
!pip install numpy --upgrade
!pip install pmdarima --no-cache-dir

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import ttest_ind
from scipy.stats import f_oneway
from scipy.stats import pearsonr, spearmanr
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error

train_ds = pd.read_csv("../content/TRAIN.csv")
test_ds = pd.read_csv("../content/TEST_FINAL.csv")

train = train_ds
test = test_ds

train.info()

test.info()

train = train.dropna().copy()  # 1 row â€” safe to drop
train['Date'] = pd.to_datetime(train['Date'])
test['Date'] = pd.to_datetime(test['Date'])

train['Store_id'] = train['Store_id'].astype(int)
train['Holiday'] = train['Holiday'].astype(int)
train['#Order'] = train['#Order'].astype(int)
test['Holiday'] = test['Holiday'].astype(int)

def add_date_parts(df):
    df['Year'] = df['Date'].dt.year
    df['Month'] = df['Date'].dt.month
    df['Week'] = df['Date'].dt.isocalendar().week
    df['Day'] = df['Date'].dt.day
    df['DayOfWeek'] = df['Date'].dt.dayofweek
    df['Is_Weekend'] = df['DayOfWeek'].isin([5,6]).astype(int)
    return df

train = add_date_parts(train)
test = add_date_parts(test)

train.info()

test.info()

train.rename(columns={'#Order': 'Orders'}, inplace=True)

train[['Date', 'Store_id', 'Store_Type', 'Location_Type', 'Region_Code',
       'Holiday', 'Discount', 'Orders', 'Sales']].sample(5)

"""# **EDA**

## **Univariate Analysis**

#### **A. Distribution of Sales and Orders**
"""

plt.figure(figsize=(14, 5))

# Sales
plt.subplot(1, 2, 1)
sns.histplot(train['Sales'], bins=50, kde=True)
plt.title("Distribution of Sales")

# Orders
plt.subplot(1, 2, 2)
sns.histplot(train['Orders'], bins=50, kde=True)
plt.title("Distribution of Orders")

plt.tight_layout()
plt.show()

"""##### **ğŸ“Š Histogram Insights**


Both Sales and Orders are right-skewed (long tail to the right).

The majority of:

Sales fall between â‚¹20,000 â€“ â‚¹60,000.

Orders mostly between 30 â€“ 100 per day.

There's a steep drop after those values, with rare outliers reaching:

Sales > â‚¹200,000

Orders > 300

ğŸ“Œ Insight:
This skew is very common in retail. We'll need to normalize/transform these variables later when training regression models (like log transformation or scaling), especially for tree-based models or linear regression.

#### **B. Boxplots of Sales to check for outliers**
"""

plt.figure(figsize=(8, 4))
sns.boxplot(x=train['Sales'])
plt.title("Boxplot of Sales")
plt.show()

"""#### **Boxplot Insights**

Significant number of outliers beyond the upper whisker (typically 1.5Ã—IQR).

The box (IQR) spans roughly â‚¹30k â€“ â‚¹50k.

Outliers exist up to â‚¹2L+.

ğŸ“Œ Insight:

Outliers are expected in a large retail dataset â€” some stores or promotions will spike sales.

We wonâ€™t drop them blindly; instead:

Consider capping (winsorization) for some models

Use robust metrics (median, IQR) for dashboard summaries
"""

train.describe(include="all").T

"""| Feature         | Insight                                                              |
| --------------- | -------------------------------------------------------------------- |
| `Sales`         | Mean \~ â‚¹42k, median \~ â‚¹39k, max â‚¹2.2L â†’ right-skewed with outliers |
| `Orders`        | Mean \~ 68 orders/day, max = 347 â†’ matches histogram                 |
| `Store_Type`    | 4 categories, with **S1 most common** (\~80k out of \~169k)          |
| `Location_Type` | 5 types; **L1 dominates** (\~76k)                                    |
| `Region_Code`   | Only 4 unique values â†’ can be grouped/bar-charted                    |
| `Discount`      | Balanced but slightly more `No` â†’ \~55% no-discount days             |
| `Holiday`       | Only \~13% of the days are holidays                                  |
| `Date Range`    | From Jan 2018 to April 2019 â€” around 1 year + 3 months               |

#### **C. Categorical Counts (Store Type, Location Type, Region)**
"""

cat_cols = ['Store_Type', 'Location_Type', 'Region_Code', 'Discount', 'Holiday']

for col in cat_cols:
    plt.figure(figsize=(6, 4))
    sns.countplot(data=train, x=col, order=train[col].value_counts().index)
    plt.title(f"Count of {col}")
    plt.xticks(rotation=45)
    plt.show()

"""#### **Count plot Insights**

**Count of Store_Type**

S1 stores dominate the data (80k+ rows), followed by S4, S2, and S3.

That means the model may learn patterns mostly from S1 â€” ensure you handle this imbalance carefully during training and evaluation.

ğŸ“Œ Action: Group-level analysis (sales per store type) is important to avoid bias.

**Count of Location_Type**

L1 is most common, followed by L2 â†’ L5.

The lower frequency of L4 and L5 might mean those segments are less statistically stable in modeling.

ğŸ“Œ Action: You might want to merge underrepresented levels later (L4, L5) if they create modeling noise or if performance is unstable.

**Count of Region_Code**

Slightly more balanced than above â€” R1 > R2 > R3 > R4.

Still, R4 is less than half of R1 â€” might impact region-wise modeling.

ğŸ“Œ Insight: Regional sales modeling could still be effective due to diversity in region codes.

**Count of Discount**

Slightly more No Discount days (~93k) vs Discount (~76k).

This is good â€” the dataset has a healthy balance to study discount impact.

ğŸ“Œ Insight: This will be useful for hypothesis testing (t-test) later.

**Count of Holiday**

Only about 13% of days are holidays, rest are regular days.

We might expect higher average sales during holidays, but we'll need statistical confirmation.

ğŸ“Œ Action: Include holidays in all grouped time-based aggregations (sales, orders).

### **Time Series Plots**
"""

daily_sales = train.groupby('Date')[['Sales', 'Orders']].sum().reset_index()

plt.figure(figsize=(14, 6))
sns.lineplot(data=daily_sales, x='Date', y='Sales')
plt.title("Daily Sales Over Time")
plt.show()

plt.figure(figsize=(14, 6))
sns.lineplot(data=daily_sales, x='Date', y='Orders')
plt.title("Daily Orders Over Time")
plt.show()

"""#### **Insights**

**Daily Sales Over Time**

Y-axis: ~â‚¹1 crore to â‚¹2.5 crore (1e7 = 10,000,000).

Data spans Jan 2018 to April 2019.

Appears to have weekly cycles â€” dips followed by quick recoveries.

Sharp drops at regular intervals â€” possibly Sundays, holidays, or stock outages.

Peaks seem more frequent mid-year and during December â€” consistent with monthly seasonality.

ğŸ“Œ Insights:

Seasonality is clearly present â€” especially in months 6â€“7 and 12 (as noted earlier).

Anomalies (dips or spikes) may correspond to special promo days or missing data. Worth tagging or checking.

**Daily Orders Over Time**

Same timeline and shape as sales, but with lower Y-scale (~40,000 orders max).

Strong correlation to sales â€” visible in simultaneous dips/spikes.

Shows slightly more stability than sales â€” possibly because individual order value may vary while count is steady.

ğŸ“Œ Insights:

Number of orders is a strong driver of sales â†’ confirm this statistically using correlation.

Also shows periodic dips, again likely weekends or holidays.

### **Grouped Averages**

#### **A. Average Sales by Store Type and Location**
"""

grouped = train.groupby(['Store_Type', 'Location_Type'])['Sales'].mean().reset_index()

plt.figure(figsize=(10, 5))
sns.barplot(data=grouped, x='Store_Type', y='Sales', hue='Location_Type')
plt.title("Average Sales by Store and Location Type")
plt.show()

"""#### **Insights**

Average Sales by Store_Type and Location_Type

ğŸ’¡ Very informative!

S4 in L2 has the highest average sales (~â‚¹63k+)

For each store type:

L2 consistently leads in average sales

L4 and L5 consistently trail

ğŸ“Œ Insights:

Store and location type interact significantly â†’ suggests importance of interaction features.

L2 might be a high-income or high-footfall zone â€” this is a strong predictor.

#### **B. Sales by Month**
"""

monthly = train.groupby('Month')['Sales'].mean().reset_index()

plt.figure(figsize=(8, 4))
sns.barplot(data=monthly, x='Month', y='Sales')
plt.title("Average Monthly Sales")
plt.show()

"""#### **Insights**

Average Monthly Sales

Sales peak in July and December (month 7 and 12)

Dip in months 8 (Aug), 10 (Oct), 11 (Nov)

ğŸ“Œ Insights:

There is clear seasonality in the data.

July and December likely see festive/holiday sales bumps.
"""

plt.figure(figsize=(10, 5))
sns.boxplot(data=train, x='Store_Type', y='Sales')
plt.title("Sales Distribution by Store Type")
plt.show()

plt.figure(figsize=(10, 5))
sns.boxplot(data=train, x='Location_Type', y='Sales')
plt.title("Sales Distribution by Location Type")
plt.show()

plt.figure(figsize=(6, 4))
sns.boxplot(data=train, x='Discount', y='Sales')
plt.title("Sales on Discount vs No Discount Days")
plt.show()

plt.figure(figsize=(6, 4))
sns.boxplot(data=train, x='Holiday', y='Sales')
plt.title("Sales on Holidays vs Non-Holidays")
plt.xticks([0, 1], ['No', 'Yes'])
plt.show()

"""#### **Insights**

**Sales Distribution by Store Type**

What we see:

S4 stores have the highest median and widest spread â€” these seem like premium or high-traffic stores.

S2 stores show the lowest median sales and the most compact distribution.

All store types have significant outliers â€” expected due to promo days or seasonal surges.

ğŸ“Œ Takeaway:

Store_Type is strongly associated with sales.

Include it in modeling â€” and you may also want to create dummy variables (OHE) or even consider a model per store type later.


**Sales by Location Type**

Observations:

L2 leads significantly in both median and upper quartile.

L3, L1 show similar ranges (but L1 has more high outliers).

L4 and L5 have low and tight distributions.

ğŸ“Œ Takeaway:

Another strong predictor.

Consider grouping L4 & L5 if modeling suffers due to class imbalance (theyâ€™re low in count too).


**Sales on Discount vs No Discount Days**

Whatâ€™s clear:

Sales on Discount days are consistently higher.

Median sales, IQR, and even high outliers are all greater under discounts.

ğŸ“Œ Takeaway:

Visual pattern suggests significant difference â†’ weâ€™ll statistically validate this via a t-test.

**Sales on Holiday vs Non-Holiday Days**

What we see:

Slightly higher spread and upper outliers on holidays.

But the median seems only marginally different.

ğŸ“Œ Takeaway:

Visually, holiday impact is less pronounced than discounts.

Still worth testing â€” weâ€™ll confirm statistically if itâ€™s significant.

### **Weekly & Day-of-Week Analysis**

#### **Average Sales by Day of Week**
"""

day_map = {0: 'Mon', 1: 'Tue', 2: 'Wed', 3: 'Thu', 4: 'Fri', 5: 'Sat', 6: 'Sun'}
train['DayName'] = train['DayOfWeek'].map(day_map)
plt.figure(figsize=(8, 5))
sns.barplot(data=train, x='DayName', y='Sales', order=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])
plt.title("Average Sales by Day of the Week")
plt.xlabel("Day")
plt.ylabel("Average Sales")
plt.show()

"""#### **Average Sales by Day of the Week**

Sunday and Saturday have the highest average sales (~â‚¹49K and â‚¹46K).

Friday is the lowest â€” ~â‚¹39K.

Monday is slightly better than mid-week days.

ğŸ“Œ Insight:

Clear weekend uplift in sales â€” this is common in B2C retail.

Use DayOfWeek or Is_Weekend as a feature in your model.

#### **Average Sales by Week Number**
"""

train['Week'] = train['Week'].astype(int)

weekly_sales = train.groupby('Week')['Sales'].mean().reset_index()

plt.figure(figsize=(12, 5))
sns.lineplot(data=weekly_sales, x='Week', y='Sales')
plt.title("Average Sales by Week Number (1â€“52)")
plt.xlabel("Week Number")
plt.ylabel("Average Sales")
plt.show()

"""#### **Average Sales by Week Number**

Spikes in Week 27 and Week 52 (mid-year and year-end).

Trough in Week 46 (~â‚¹26K) â€” could correspond to a low-demand period.

Variability shows possible impact of promotions or events.

ğŸ“Œ Insight:

There's a strong seasonal signal in certain weeks.

Feature like Week or Week_Sin/Cos (cyclical encoding) may help.

### **Heatmaps (Calendar-style)**

#### **Heatmap: Sales by Month vs Day of Week**
"""

pivot = train.pivot_table(index='Month', columns='DayName', values='Sales', aggfunc='mean')

# Reorder columns
pivot = pivot[['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']]

plt.figure(figsize=(10, 6))
sns.heatmap(pivot, cmap="YlGnBu", annot=True, fmt=".0f")
plt.title("Average Sales by Month and Day of Week")
plt.xlabel("Day of Week")
plt.ylabel("Month")
plt.show()

"""#### **Heatmap: Sales by Month and Day of Week**

Weekends (Sat, Sun) are higher across almost all months.

May, July, and December show strong performance across all days.

Worst-performing month is August, especially mid-week (Tueâ€“Thu).

ğŸ“Œ Insight:

The combination of temporal features + day-of-week gives excellent predictive strength.

Useful for both modeling and Tableau storytelling.

#### **Heatmap: Sales by Month vs Store Type**
"""

pivot2 = train.pivot_table(index='Month', columns='Store_Type', values='Sales', aggfunc='mean')

plt.figure(figsize=(10, 6))
sns.heatmap(pivot2, cmap="Oranges", annot=True, fmt=".0f")
plt.title("Average Sales by Month and Store Type")
plt.xlabel("Store Type")
plt.ylabel("Month")
plt.show()

"""#### **Heatmap: Sales by Month vs Store Type**

S4 consistently dominates across all months (â‚¹65K+ avg).

S3 peaks in July and December, echoing earlier monthly patterns.

S2 lags behind across all months.

ğŸ“Œ Insight:

Confirms earlier boxplot findings.

Seasonal impact is store-type-dependent â†’ interaction effect is real and useful!

### **Summary**

| Feature        | Action                                                      |
| -------------- | ----------------------------------------------------------- |
| Store\_Type    | Use in modeling; S1 dominant                                |
| Location\_Type | Use as categorical feature; L2 most lucrative               |
| Region\_Code   | Moderate balance; may help region-specific strategies       |
| Discount       | Suitable for hypothesis testing                             |
| Holiday        | Rare, but likely impactful                                  |
| Interactions   | **Store\_Type Ã— Location\_Type** should be modeled          |
| Seasonality    | Time series modeling must include **Month**, **Week**, etc. |

## **Hypothesis Testing - Assess which features are strong contributors statistically**

### **Hypothesis 1: Do Discounts Lead to Higher Sales?**
Formulation:

**Null (Hâ‚€)** : Mean sales on discount days = mean sales on non-discount days

**Alternate (Hâ‚)** : Mean sales on discount days â‰  mean sales on non-discount days
"""

sales_discount = train[train['Discount'] == 'Yes']['Sales']
sales_no_discount = train[train['Discount'] == 'No']['Sales']

t_stat, p_val = ttest_ind(sales_discount, sales_no_discount, equal_var=False)

print(f"T-statistic: {t_stat:.4f}")
print(f"P-value: {p_val:.4f}")
print("Avg Sales on Discount Days:", round(sales_discount.mean(), 2))
print("Avg Sales on Non-Discount Days:", round(sales_no_discount.mean(), 2))

"""#### **Interpretation:**

Since p-value < 0.05, we reject the null hypothesis.

There is a statistically significant difference in sales between discount and non-discount days.

The average sales are ~â‚¹12,000 higher on discount days.

**This confirms what we observed visually â€” discounts significantly boost sales.**

### **Hypothesis Test 2: Holiday vs Non-Holiday Days**

**Null (Hâ‚€)** : Sales on holidays = Sales on non-holidays

**Alternate (Hâ‚)** : Sales on holidays â‰  Sales on non-holidays
"""

sales_holiday = train[train['Holiday'] == 1]['Sales']
sales_nonholiday = train[train['Holiday'] == 0]['Sales']

t_stat_holiday, p_val_holiday = ttest_ind(sales_holiday, sales_nonholiday, equal_var=False)

print(f"T-statistic: {t_stat_holiday:.4f}")
print(f"P-value: {p_val_holiday:.4f}")
print("Avg Sales on Holidays:", round(sales_holiday.mean(), 2))
print("Avg Sales on Non-Holidays:", round(sales_nonholiday.mean(), 2))

"""#### **Interpretation:**

Since p-value < 0.05, we reject the null hypothesis.

There is a statistically significant difference in sales between holiday and non-holiday days.

However, unlike common assumptions, sales are significantly lower on holidays in this dataset.

**Possible Explanations:**

Stores might be closed or partially operating on holidays.

Customers may not shop during holidays in this context (e.g., non-essential retail).

Need domain context (like store policy or product type) to explain.

*Can be inferred better in tableau*

### **Hypothesis Test 3: Do Sales Differ by Store Type?**

We now check if sales differ significantly across S1, S2, S3, S4 using ANOVA.

**Hâ‚€** : All store types have the same average sales

**Hâ‚** : At least one store type differs significantly
"""

groups = [group['Sales'].values for name, group in train.groupby('Store_Type')]
f_stat, p_val = f_oneway(*groups)

print(f"F-statistic: {f_stat:.4f}")
print(f"P-value: {p_val:.4f}")
print(train.groupby('Store_Type')['Sales'].mean())

"""#### **Interpretation:**

Since p-value < 0.05, we reject the null hypothesis.

There is a statistically significant difference in sales across store types.

S4 stores significantly outperform others, followed by S3.

**This supports what we saw visually â€” store type is a strong predictor for modeling.**

*We can use this in Tableau to segment sales and forecast potential.*

### **Hypothesis Test 4: Do Sales Vary Across Regions?**

Region codes: R1, R2, R3, R4 â€” categorical, but with no natural ordering.

Let's test this with ANOVA first, and if the data is not normal, we'll try Kruskal-Wallis.

**Hâ‚€** : All regions have the same average sales

**Hâ‚** : At least one region has different sales
"""

region_groups = [group['Sales'].values for name, group in train.groupby('Region_Code')]
f_stat_reg, p_val_reg = f_oneway(*region_groups)

print(f"F-statistic: {f_stat_reg:.4f}")
print(f"P-value: {p_val_reg:.4f}")
print(train.groupby('Region_Code')['Sales'].mean())

"""#### **Interpretation:**

Since p < 0.05, we reject the null hypothesis.

There is a significant difference in sales across regions.

R1 outperforms all other regions significantly.

**Region is another important categorical predictor to include in modeling and dashboard slicing.**

### **Correlation Between Orders and Sales**

**Hâ‚€** : No correlation between number of orders and total sales

**Hâ‚** : There is a significant correlation

We'll test both:

**Pearson:** for linear correlation (sensitive to outliers)

**Spearman:** for monotonic relationship (rank-based, robust to outliers)
"""

# Pearson correlation
pearson_corr, pearson_p = pearsonr(train['Orders'], train['Sales'])
print(f"Pearson Correlation: {pearson_corr:.4f}, P-value: {pearson_p:.4f}")

# Spearman correlation
spearman_corr, spearman_p = spearmanr(train['Orders'], train['Sales'])
print(f"Spearman Correlation: {spearman_corr:.4f}, P-value: {spearman_p:.4f}")

"""#### **Interpretation:**

Both Pearson and Spearman correlations are > 0.93 â†’ this is a very strong positive relationship.

Since p < 0.05, the correlations are statistically significant.

**This confirms a strong, monotonic and linear relationship:**

**As the number of orders increases, total sales increase consistently and strongly.**

We can:

Use Orders as a direct feature

Consider it for creating derived features like Sales per Order, etc.

### **Final Summary: Hypothesis Testing Block**

| Hypothesis         | Result               | Insight                           |
| ------------------ | -------------------- | --------------------------------- |
| ğŸ’¸ Discount impact | âœ… Significant        | Sales **â†‘ â‚¹12k** on discount days |
| ğŸ‰ Holiday impact  | âœ… Significant        | Sales **â†“ â‚¹8k** on holidays       |
| ğŸ¬ Store Type      | âœ… Significant        | S4 > S3 > S1 > S2                 |
| ğŸŒ Region Code     | âœ… Significant        | R1 > R3 > R2 > R4                 |
| ğŸ“¦ Orders vs Sales | âœ… Strong correlation | r â‰ˆ 0.94                          |

# **Model Preparation**

## **Drop Columns**
"""

test.head()

cols_to_drop = ['ID', 'Date']
train_model = train.drop(columns=cols_to_drop)
train_model = train_model.drop(columns=['DayName'])
test_model = test.drop(columns=cols_to_drop)

train_model.columns

"""## **Encode Categorical Variables**

Categorical columns:

Store_Type

Location_Type

Region_Code

Discount (Yes/No)

Let us use One-Hot Encoding for simplicity (since most have low cardinality).
"""

cat_cols = ['Store_Type', 'Location_Type', 'Region_Code', 'Discount']
train_model = pd.get_dummies(train_model, columns=cat_cols, drop_first=True)
test_model = pd.get_dummies(test_model, columns=cat_cols, drop_first=True)
train_model, test_model = train_model.align(test_model, join='left', axis=1, fill_value=0)

"""## **Train-Test Split (Time Based)**

We'll simulate a real forecasting scenario by splitting by date â€” not random split.

Let's take:

Train: All data before 2019

Validation: Janâ€“April 2019
"""

train['Date'] = pd.to_datetime(train['Date'])
train_cutoff = pd.to_datetime('2019-01-01')
mask_train = train['Date'] < train_cutoff
mask_val = train['Date'] >= train_cutoff

X_train = train_model.loc[mask_train].drop(columns='Sales')
y_train = train_model.loc[mask_train]['Sales']
X_val = train_model.loc[mask_val].drop(columns='Sales')
y_val = train_model.loc[mask_val]['Sales']

print("X_train:", X_train.shape)
print("y_train:", y_train.shape)
print("X_val:", X_val.shape)
print("y_val:", y_val.shape)

"""# **Model Training & Evaluation**

## **Baseline Model â€” Linear Regression**

Let us
* Train the model
* Predict on validation set

Evaluate using:

* MAE (Mean Absolute Error)
* RMSE (Root Mean Squared Error)
* MAPE (Mean Absolute Percentage Error)
"""

X_train.head()

lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
y_pred = lr_model.predict(X_val)

mae = mean_absolute_error(y_val, y_pred)
rmse = np.sqrt(mean_squared_error(y_val, y_pred))
# mape = mean_absolute_percentage_error(y_val, y_pred)
non_zero_mask = y_val != 0
mape = np.mean(np.abs((y_val[non_zero_mask] - y_pred[non_zero_mask]) / y_val[non_zero_mask])) * 100

print(f"Linear Regression Performance:")
print(f"MAE:  â‚¹{mae:.2f}")
print(f"RMSE: â‚¹{rmse:.2f}")
print(f"MAPE: {mape:.2f}%")

"""#### **Plot Predictions vs Actual**"""

plt.figure(figsize=(8,5))
plt.scatter(y_val, y_pred, alpha=0.3)
plt.xlabel("Actual Sales")
plt.ylabel("Predicted Sales")
plt.title("Actual vs Predicted Sales (Linear Regression)")
plt.show()

"""#### **Save Predictions for Tableau**"""

val_results = X_val.copy()
val_results['Actual_Sales'] = y_val
val_results['Predicted_Sales'] = y_pred
val_results.to_csv('../content/predictions_linear.csv', index=False)

"""## **Model 2 â€” XGBoost Regressor**

Weâ€™ll now:

* Train a more powerful model: XGBoost
* Predict and evaluate using same metrics: MAE, RMSE, MAPE
* Optionally export predictions for Tableau
"""

xgb_model = XGBRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=6,
    random_state=42,
    n_jobs=-1
)

xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_val)

mae = mean_absolute_error(y_val, y_pred_xgb)
rmse = np.sqrt(mean_squared_error(y_val, y_pred_xgb))
non_zero_mask = y_val != 0
mape = np.mean(np.abs((y_val[non_zero_mask] - y_pred[non_zero_mask]) / y_val[non_zero_mask])) * 100

print("XGBoost Model Performance:")
print(f"MAE: â‚¹{mae:.2f}")
print(f"RMSE: â‚¹{rmse:.2f}")
print(f"MAPE: {mape:.2f}%")

plt.figure(figsize=(8,5))
plt.scatter(y_val, y_pred_xgb, alpha=0.3)
plt.xlabel("Actual Sales")
plt.ylabel("Predicted Sales")
plt.title("Actual vs Predicted Sales (XGBoost)")
plt.show()

val_results = X_val.copy()
val_results['Actual_Sales'] = y_val
val_results['Predicted_Sales'] = y_pred_xgb

val_results.to_csv('../content/predictions_xgboost.csv', index=False)

print(f"Minimum y_val: {np.min(y_val)}")
print(f"Count of zero y_val: {(y_val == 0).sum()}")

"""### **Model Performance Summary**

| Metric   | Linear Regression | XGBoost   |
| -------- | ----------------- | --------- |
| **MAE**  | â‚¹4116.87          | â‚¹2435.30  |
| **RMSE** | â‚¹5535.37          | â‚¹3699.50  |
| **MAPE** | **9.96%**         | **9.96%** |

#### **Interpretation:**

XGBoost outperforms Linear Regression in both MAE and RMSE, meaning it gives better predictions overall.

MAPE is the same (~9.96%) for both, likely because it's averaged over many values â€” though the variance is better handled by XGBoost.

### **Predict on Test Data and Prepare Submission**
"""

test.columns

test_model = test_model.drop(columns=["Sales"])

test_predictions = xgb_model.predict(test_model)
submission = pd.DataFrame({
    'ID': test['ID'],
    'Sales_Predicted': test_predictions
})

submission.to_csv("../content/sales_predictions.csv", index=False)
submission.head()

"""## **Time-series Forecasting**

### **Aggregate Time Series Data**
We'll resample daily sales to weekly totals to reduce noise and make trends more visible.
"""

train['Date'] = pd.to_datetime(train['Date'])

weekly_sales = (
    train.groupby(pd.Grouper(key='Date', freq='W'))['Sales']
    .sum()
    .reset_index()
    .rename(columns={'Sales': 'Weekly_Sales'})
)

weekly_sales.set_index('Date', inplace=True)

plt.figure(figsize=(12, 5))
plt.plot(weekly_sales.index, weekly_sales['Weekly_Sales'], marker='o')
plt.title('Weekly Total Sales')
plt.xlabel('Week')
plt.ylabel('Sales')
plt.grid(True)
plt.tight_layout()
plt.show()

"""#### **The weekly sales plot shows a clear seasonal pattern with spikes â€” especially toward the end of the year and early summer, which suggests potential seasonality and trend, perfect for SARIMA or Prophet later.**

### **ARIMA Modeling (Baseline)**

We'll start with ARIMA, which works on stationary time series (i.e., constant mean and variance over time). Letâ€™s proceed step-by-step:

#### **Check Stationarity (ADF Test)**
"""

from statsmodels.tsa.stattools import adfuller

result = adfuller(weekly_sales['Weekly_Sales'])
print(f"ADF Statistic: {result[0]:.4f}")
print(f"P-value: {result[1]:.4f}")

"""#### **The series is not stationary (ADF p-value = 0.0666 > 0.05), so we must difference the series before applying ARIMA.**"""

# First-order differencing
weekly_sales['Weekly_Sales_Diff'] = weekly_sales['Weekly_Sales'].diff()

# Drop NA and plot
plt.figure(figsize=(12,5))
plt.plot(weekly_sales['Weekly_Sales_Diff'].dropna(), label='1st Order Differenced Sales')
plt.title("Weekly Sales - First Order Differencing")
plt.ylabel("Differenced Sales")
plt.grid(True)
plt.legend()
plt.show()

# ADF test after differencing
result_diff = adfuller(weekly_sales['Weekly_Sales_Diff'].dropna())
print(f"ADF Statistic (Differenced): {result_diff[0]:.4f}")
print(f"P-value (Differenced): {result_diff[1]:.4f}")

"""#### **ADF p-value after differencing = 0.0000 < 0.05, so the differenced series is stationary â†’ We can now proceed to ARIMA modeling with d=1**

### **Fit ARIMA Model (Using auto_arima)**

Letâ€™s now let auto_arima figure out the best (p, d, q) parameters.
"""

from statsmodels.tsa.arima.model import ARIMA

# Fit ARIMA model
model = ARIMA(weekly_sales['Weekly_Sales'], order=(1, 1, 1))
model_fit = model.fit()

# Summary of the model
print(model_fit.summary())

"""### **Forecast with ARIMA**

Letâ€™s forecast for the next 12 weeks and plot it:
"""

# Forecast next 12 weeks
forecast = model_fit.get_forecast(steps=12)
forecast_values = forecast.predicted_mean
conf_int = forecast.conf_int()

# Dates for forecast
last_date = weekly_sales.index[-1]
future_dates = pd.date_range(start=last_date + pd.Timedelta(weeks=1), periods=12, freq='W')


# Plot
plt.figure(figsize=(12, 5))
plt.plot(weekly_sales.index, weekly_sales['Weekly_Sales'], label='Historical Sales')
plt.plot(future_dates, forecast_values, label='Forecast', color='green')
plt.fill_between(future_dates, conf_int.iloc[:, 0], conf_int.iloc[:, 1], color='lightgreen', alpha=0.5)
plt.title("ARIMA(1,1,1) Forecast for Weekly Sales")
plt.xlabel("Week")
plt.ylabel("Sales")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

"""#### **Interpretation of Results**
ğŸ“‰ Forecast Plot:

* The green line represents the point forecast for the next 12 weeks.
* The light green shaded area is the 95% confidence interval.
* The forecast seems stable (flat) â€” a common behavior with ARIMA when there's no strong trend or seasonality explicitly modeled.

#### **Model Summary Highlights:**

* AR coefficient (p=1): 0.1402, not significant (p=0.146).
* MA coefficient (q=1): -0.75, significant (p < 0.000).
* Ljung-Box test (L1): p=0.25 â†’ residuals do not show autocorrelation (good).
* JB test: p=0.47 â†’ residuals are roughly normal.
* SigmaÂ²: The variance is quite large, so we should be cautious of prediction intervals.

**However, the model is okay but not optimal because it does not model seasonality â€” that's where SARIMA or Prophet comes in.**

### **SARIMA to Model Seasonality**

Since the sales data clearly shows weekly seasonality, we will now fit a Seasonal ARIMA model (SARIMA).

* ARIMA part (p=1, d=1, q=1)
* Seasonal part (P=1, D=1, Q=1, s=52) â†’ assuming yearly weekly seasonality
"""

import warnings
import itertools
from statsmodels.tsa.statespace.sarimax import SARIMAX

# Define parameter grid
p = d = q = range(0, 2)
P = D = Q = range(0, 2)
s = 52  # weekly seasonality
pdq = list(itertools.product(p, d, q))
seasonal_pdq = list(itertools.product(P, D, Q, [s]))

best_aic = float("inf")
best_params = None

for param in pdq:
    for seasonal_param in seasonal_pdq:
        try:
            model = SARIMAX(
                weekly_sales['Weekly_Sales'],
                order=param,
                seasonal_order=seasonal_param,
                enforce_stationarity=False,
                enforce_invertibility=False
            )
            result = model.fit(disp=False)
            if result.aic < best_aic:
                best_aic = result.aic
                best_params = (param, seasonal_param)
        except:
            continue

print("Best SARIMA order:", best_params[0])
print("Best seasonal order:", best_params[1])
print("Best AIC:", best_aic)

"""### **Fit and Forecast with Best SARIMA**"""

from statsmodels.tsa.statespace.sarimax import SARIMAX
import matplotlib.pyplot as plt

best_sarima_model = SARIMAX(
    weekly_sales['Weekly_Sales'],
    order=(0, 0, 0),
    seasonal_order=(0, 1, 1, 52),
    enforce_stationarity=False,
    enforce_invertibility=False
)
best_sarima_result = best_sarima_model.fit(disp=False)

sarima_forecast = best_sarima_result.get_forecast(steps=12)
conf_int = sarima_forecast.conf_int()
predicted_mean = sarima_forecast.predicted_mean

# Plot
plt.figure(figsize=(14,6))
plt.plot(weekly_sales.index, weekly_sales['Weekly_Sales'], label='Historical Sales')
plt.plot(predicted_mean.index, predicted_mean, label='SARIMA Forecast', color='green')
plt.fill_between(predicted_mean.index, conf_int.iloc[:, 0], conf_int.iloc[:, 1], color='green', alpha=0.3)
plt.title("Final SARIMA(0,0,0)x(0,1,1,52) Weekly Sales Forecast")
plt.xlabel("Week")
plt.ylabel("Sales")
plt.legend()
plt.show()

"""#### **Summary:**

* Seasonal trend clearly captured (weekly seasonality).
* Good AIC (4.0).
* forecast looks well-aligned with past seasonal trends and the confidence intervals are reasonable.
* No hyperparameter issues encountered with manual tuning.

### **Prophet**

#### **Prepare Data**
"""

print(weekly_sales.head())
print(weekly_sales.columns)

# Reset index so 'Date' becomes a column
weekly_sales_reset = weekly_sales.reset_index()

# Rename for Prophet: 'Date' â†’ 'ds', 'Weekly_Sales' â†’ 'y'
prophet_df = weekly_sales_reset.rename(columns={'Date': 'ds', 'Weekly_Sales': 'y'})

# Ensure datetime type (important for Prophet)
prophet_df['ds'] = pd.to_datetime(prophet_df['ds'])

# Final check
prophet_df.head()

"""#### **Fit the prophet model**"""

from prophet import Prophet

prophet_model = Prophet(weekly_seasonality=True, yearly_seasonality=True, daily_seasonality=False)
prophet_model.fit(prophet_df)

"""#### **Create Future Dates and Forecast**"""

# Forecast next 12 weeks
future = prophet_model.make_future_dataframe(periods=12, freq='W')
forecast = prophet_model.predict(future)

# Plot forecast
fig1 = prophet_model.plot(forecast)
plt.title('Prophet Forecast for Weekly Sales')
plt.show()

# Optional: component plots (trend, seasonality)
fig2 = prophet_model.plot_components(forecast)
plt.show()

"""#### **Visual Insights**

**ARIMA(1,1,1):**

* Flat forecast line.
* Confidence intervals widen over time, indicating growing uncertainty.
* Not suitable if seasonality exists (which clearly does in your case).

**SARIMA(0,0,0)(0,1,1,52):**

* Captures seasonal fluctuations reasonably well (weekly seasonality of 52).
* Forecast fluctuates to reflect past seasonal behavior.
* Confidence intervals still widen but the pattern is more consistent with real sales history.

**Prophet:**

* Strong seasonality decomposition:
* Trend component shows a slight declining pattern.
* Weekly seasonality: Highest sales on Sundays, lowest mid-week.
* Yearly seasonality: Peaks around Jan, May, July; dips near Nov.
* Forecast aligns closely with observed trends.
* Handles missing dates and irregular time series without preprocessing.

#### **âœ… Recommendation â€“ Best Model**

ğŸ“Œ Prophet is the best-suited model for your weekly sales forecasting due to:

* Its automated handling of trend, multiple seasonality levels, and holidays (can be added).
* Interpretability via component plots.
* No need to manually tune p,d,q seasonal orders.

**Alternative: Use SARIMA if:**

* If we prefer classical statistical models and have consistent weekly data with strong yearly cycles.
* We want full manual control over seasonality and differencing.

# **Save the best models**
"""

import pickle

# Save the Prophet model
with open('../content/prophet_model.pkl', 'wb') as f:
    pickle.dump(prophet_model, f)

print("Prophet model saved as prophet_model.pkl")

# Save the XGBoost model
with open('../content/xgb_model.pkl', 'wb') as f:
    pickle.dump(xgb_model, f)

print("XGBoost model saved as xgb_model.pkl")